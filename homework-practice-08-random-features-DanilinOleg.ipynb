{"cells":[{"cell_type":"markdown","metadata":{"id":"RYp0bXOFK-hP"},"source":["# Машинное обучение, ФКН ВШЭ\n","\n","## Практическое задание 8. Метод опорных векторов и аппроксимация ядер\n","\n","### Общая информация\n","\n","Дата выдачи: 02.02.2024\n","\n","Мягкий дедлайн: 23:59MSK 19.02.2024\n","\n","Жесткий дедлайн: 23:59MSK 25.02.2024\n","\n","### Оценивание и штрафы\n","Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимальная оценка за работу (без учёта бонусов) — 10 баллов.\n","\n","Сдавать задание после указанного жёсткого срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n","\n","Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n","\n","Неэффективная реализация кода может негативно отразиться на оценке.\n","\n","### Формат сдачи\n","Задания сдаются через систему anytask. Посылка должна содержать:\n","* Ноутбук homework-practice-08-random-features-Username.ipynb\n","\n","Username — ваша фамилия и имя на латинице именно в таком порядке"]},{"cell_type":"markdown","metadata":{"id":"vY8vT0W_K-hR"},"source":["### О задании\n","\n","На занятиях мы подробно обсуждали метод опорных векторов (SVM). В базовой версии в нём нет чего-то особенного — мы всего лишь используем специальную функцию потерь, которая не требует устремлять отступы к бесконечности; ей достаточно, чтобы отступы были не меньше +1. Затем мы узнали, что SVM можно переписать в двойственном виде, который, позволяет заменить скалярные произведения объектов на ядра. Это будет соответствовать построению модели в новом пространстве более высокой размерности, координаты которого представляют собой нелинейные модификации исходных признаков.\n","\n","Ядровой SVM, к сожалению, довольно затратен по памяти (нужно хранить матрицу Грама размера $d \\times d$) и по времени (нужно решать задачу условной оптимизации с квадратичной функцией, а это не очень быстро). Мы обсуждали, что есть способы посчитать новые признаки $\\tilde \\varphi(x)$ на основе исходных так, что скалярные произведения этих новых $\\langle \\tilde \\varphi(x), \\tilde \\varphi(z) \\rangle$ приближают ядро $K(x, z)$.\n","\n","Мы будем исследовать аппроксимации методом Random Fourier Features (RFF, также в литературе встречается название Random Kitchen Sinks) для гауссовых ядер. Будем использовать формулы, которые немного отличаются от того, что было на лекциях (мы добавим сдвиги внутрь тригонометрических функций и будем использовать только косинусы, потому что с нужным сдвигом косинус превратится в синус):\n","$$\\tilde \\varphi(x) = (\n","\\cos (w_1^T x + b_1),\n","\\dots,\n","\\cos (w_n^T x + b_n)\n","),$$\n","где $w_j \\sim \\mathcal{N}(0, 1/\\sigma^2)$, $b_j \\sim U[-\\pi, \\pi]$.\n","\n","На новых признаках $\\tilde \\varphi(x)$ мы будем строить любую линейную модель.\n","\n","Можно считать, что это некоторая новая парадигма построения сложных моделей. Можно направленно искать сложные нелинейные закономерности в данных с помощью градиентного бустинга или нейронных сетей, а можно просто нагенерировать большое количество случайных нелинейных признаков и надеяться, что быстрая и простая модель (то есть линейная) сможет показать на них хорошее качество. В этом задании мы изучим, насколько работоспособна такая идея.\n","\n","### Алгоритм\n","\n","Вам потребуется реализовать следующий алгоритм:\n","1. Понизить размерность выборки до new_dim с помощью метода главных компонент.\n","2. Для полученной выборки оценить гиперпараметр $\\sigma^2$ с помощью эвристики (рекомендуем считать медиану не по всем парам объектов, а по случайному подмножеству из где-то миллиона пар объектов): $$\\sigma^2 = \\text{median}_{i, j = 1, \\dots, \\ell, i \\neq j} \\left\\{\\sum_{k = 1}^{d} (x_{ik} - x_{jk})^2 \\right\\}$$\n","3. Сгенерировать n_features наборов весов $w_j$ и сдвигов $b_j$.\n","4. Сформировать n_features новых признаков по формулам, приведённым выше.\n","5. Обучить линейную модель (логистическую регрессию или SVM) на новых признаках.\n","6. Повторить преобразования (PCA, формирование новых признаков) к тестовой выборке и применить модель."]},{"cell_type":"markdown","metadata":{"id":"N_sGunb7K-hS"},"source":["Тестировать алгоритм мы будем на данных Fashion MNIST. Ниже код для их загрузки и подготовки."]},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"kxE0oUPTtm52","executionInfo":{"status":"ok","timestamp":1708374697475,"user_tz":-180,"elapsed":306,"user":{"displayName":"БИ Двестидвенадцать","userId":"11604785324055586986"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","execution_count":44,"metadata":{"id":"YyG6dBfjK-hS","executionInfo":{"status":"ok","timestamp":1708374698737,"user_tz":-180,"elapsed":3,"user":{"displayName":"БИ Двестидвенадцать","userId":"11604785324055586986"}}},"outputs":[],"source":["import keras\n","from keras.datasets import fashion_mnist\n","(x_train_pics, y_train), (x_test_pics, y_test) = fashion_mnist.load_data()\n","x_train = x_train_pics.reshape(y_train.shape[0], -1)\n","x_test = x_test_pics.reshape(y_test.shape[0], -1)"]},{"cell_type":"markdown","metadata":{"id":"rJNN55F7K-hT"},"source":["__Задание 1. (5 баллов)__\n","\n","Реализуйте алгоритм, описанный выше. Можете воспользоваться шаблоном класса ниже или написать свой интерфейс.\n","\n","Ваша реализация должна поддерживать следующие опции:\n","1. Возможность задавать значения гиперпараметров new_dim (по умолчанию 50) и n_features (по умолчанию 1000).\n","2. Возможность включать или выключать предварительное понижение размерности с помощью метода главных компонент.\n","3. Возможность выбирать тип линейной модели (логистическая регрессия или SVM с линейным ядром).\n","\n","Протестируйте на данных Fashion MNIST, сформированных кодом выше. Если на тесте у вас получилась доля верных ответов не ниже 0.84 с гиперпараметрами по умолчанию, то вы всё сделали правильно."]},{"cell_type":"code","execution_count":45,"metadata":{"id":"jP8yepx8K-hT","executionInfo":{"status":"ok","timestamp":1708374700109,"user_tz":-180,"elapsed":279,"user":{"displayName":"БИ Двестидвенадцать","userId":"11604785324055586986"}}},"outputs":[],"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.linear_model import LogisticRegression\n","\n","from sklearn import svm\n","from sklearn.preprocessing import StandardScaler, Normalizer\n","\n","\n","\n","\n","class RFFPipeline(BaseEstimator, TransformerMixin):\n","    def __init__(self, n_features=1000, new_dim=50, use_PCA=True, classifier='logreg', original = False):\n","        \"\"\"\n","        Implements pipeline, which consists of PCA decomposition,\n","        Random Fourier Features approximation and linear classification model.\n","\n","        n_features, int: amount of synthetic random features generated with RFF approximation.\n","\n","        new_dim, int: PCA output size.\n","\n","        use_PCA, bool: whether to include PCA preprocessing.\n","\n","        classifier, string: either 'svm' or 'logreg', a linear classification model to use on top of pipeline.\n","\n","        Feel free to edit this template for your preferences.\n","        \"\"\"\n","        self.n_features = n_features\n","        self.use_PCA = use_PCA\n","        self.new_dim = new_dim\n","        self.classifier = classifier\n","        self.model = LogisticRegression()\n","        self.red = PCA(n_components = new_dim)\n","        self.normalizer = Normalizer()\n","        self.original = original\n","        self.w = None\n","        self.b = None\n","\n","        if self.classifier == 'svm_linear':\n","          self.model = svm.LinearSVC()\n","        elif self.classifier == 'svm_kernel':\n","          self.model = svm.SVC(kernel='rbf')\n","\n","\n","\n","\n","\n","    def transform(self, X):\n","      self.normalizer.fit(X)\n","      X = self.normalizer.transform(X)\n","      self.red.fit(X)\n","      X = self.red.transform(X)\n","\n","      return X\n","\n","    def fit(self, X, y):\n","        \"\"\"\n","        Fit all parts of algorithm (PCA, RFF, Classification) to training set.\n","        \"\"\"\n","        self.normalizer.fit(X)\n","        X = self.normalizer.transform(X)\n","\n","        if self.use_PCA:\n","          self.red.fit(X)\n","          X = self.red.transform(X)\n","\n","        # генерируем миллион пар и оцениваем сигму\n","        if self.original == False:\n","          indices_1 = np.random.choice(X.shape[0], int(1000100 / (1 + self.new_dim - 50))) # мы уменьшаем количество сэмплов, если new_dim будет больше 50\n","          indices_2 = np.random.choice(X.shape[0], int(1000100 / (1 + self.new_dim - 50)))\n","          result = np.sum((X[indices_1] - X[indices_2]) ** 2, axis = 1)\n","          sigma = np.median(result[result != 0])\n","\n","          self.w = np.random.normal(0, 1 / sigma, size = (self.n_features, self.new_dim))\n","          self.b = np.random.uniform(-np.pi, np.pi, self.n_features)\n","\n","          X = np.cos(X @ self.w.T + self.b)\n","\n","        self.model.fit(X, y)\n","\n","        return self\n","\n","    def predict_proba(self, X):\n","        \"\"\"\n","        Apply pipeline to obtain scores for input data.\n","        \"\"\"\n","        raise NotImplementedError\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Apply pipeline to obtain discrete predictions for input data.\n","        \"\"\"\n","        X = self.normalizer.transform(X)\n","        if self.use_PCA:\n","          X = self.red.transform(X)\n","        if self.original == False:\n","          X = np.cos(X @ self.w.T + self.b)\n","\n","        return self.model.predict(X)"]},{"cell_type":"code","source":["# протестируем на logreg\n","%%time\n","rff_logreg = RFFPipeline()\n","rff_logreg.fit(x_train, y_train)\n","pred = rff_logreg.predict(x_test)\n","accuracy_score(y_test, pred)"],"metadata":{"id":"_tCjnw7hyAUv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708374776227,"user_tz":-180,"elapsed":69837,"user":{"displayName":"БИ Двестидвенадцать","userId":"11604785324055586986"}},"outputId":"599f5be9-149a-493c-e9e0-49f5c7dccbc3"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 1min 27s, sys: 18.1 s, total: 1min 45s\n","Wall time: 1min 9s\n"]},{"output_type":"execute_result","data":{"text/plain":["0.8644"]},"metadata":{},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"HYqQUEi-K-hU"},"source":["__Задание 2. (3 балла)__\n","\n","Сравните подход со случайными признаками с обучением SVM на исходных признаках. Попробуйте вариант с обычным (линейным) SVM и с ядровым SVM. Ядровой SVM может очень долго обучаться, поэтому можно делать любые разумные вещи для ускорения: брать подмножество объектов из обучающей выборки, например.\n","\n","Сравните подход со случайными признаками с вариантом, в котором вы понижаете размерность с помощью PCA и обучаете градиентный бустинг. Используйте одну из реализаций CatBoost/LightGBM/XGBoost, не забудьте подобрать число деревьев и длину шага.\n","\n","Сделайте выводы — насколько идея со случайными признаками работает? Сравните как с точки зрения качества, так и с точки зрения скорости обучения и применения."]},{"cell_type":"markdown","source":["**Logreg на исходных**"],"metadata":{"id":"KY8NXchvSvWF"}},{"cell_type":"code","source":["%%time\n","rff_logreg = RFFPipeline(use_PCA=False, original = True)\n","rff_logreg.fit(x_train, y_train)\n","pred = rff_logreg.predict(x_test)\n","accuracy_score(y_test, pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tJsPQ5d0SoP_","executionInfo":{"status":"ok","timestamp":1708374816584,"user_tz":-180,"elapsed":34583,"user":{"displayName":"БИ Двестидвенадцать","userId":"11604785324055586986"}},"outputId":"77add08e-76f0-46a1-c24a-313027f2f981"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 53.5 s, sys: 5.34 s, total: 58.8 s\n","Wall time: 34.2 s\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"execute_result","data":{"text/plain":["0.837"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","source":["**Линейный SVM на случайных признаках**\n","\n","Пробовал обучить на всей выборке, очень медленно, ограничивается итерациями, поэтому возьму подвыборку из 15000"],"metadata":{"id":"rLC2WWANtjGp"}},{"cell_type":"code","source":["indices = np.random.choice(x_train.shape[0], 15000)"],"metadata":{"id":"RSSwWppNzNjD","executionInfo":{"status":"ok","timestamp":1708374816585,"user_tz":-180,"elapsed":33,"user":{"displayName":"БИ Двестидвенадцать","userId":"11604785324055586986"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["%%time\n","rff_svm_linear_pca = RFFPipeline(classifier = 'svm_linear', use_PCA=True, original = False)\n","rff_svm_linear_pca.fit(x_train[indices], y_train[indices])\n","pred = rff_svm_linear_pca.predict(x_test)\n","accuracy_score(y_test, pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y7gwv4E0tob8","executionInfo":{"status":"ok","timestamp":1708374907017,"user_tz":-180,"elapsed":90464,"user":{"displayName":"БИ Двестидвенадцать","userId":"11604785324055586986"}},"outputId":"a8f322b8-708e-47d0-c25d-a808948c4129"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 1min 25s, sys: 2.9 s, total: 1min 28s\n","Wall time: 1min 30s\n"]},{"output_type":"execute_result","data":{"text/plain":["0.8515"]},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","source":["**Линейный SVM на исходных признаках**"],"metadata":{"id":"pUyDOg2sUCGT"}},{"cell_type":"code","execution_count":50,"metadata":{"id":"qN8LUlJgK-hV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708374912972,"user_tz":-180,"elapsed":5962,"user":{"displayName":"БИ Двестидвенадцать","userId":"11604785324055586986"}},"outputId":"4788335c-8a4e-44ff-d273-2510b2f36c0d"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 5.8 s, sys: 194 ms, total: 5.99 s\n","Wall time: 5.94 s\n"]},{"output_type":"execute_result","data":{"text/plain":["0.8341"]},"metadata":{},"execution_count":50}],"source":["%%time\n","rff_svm_linear = RFFPipeline(classifier = 'svm_linear', use_PCA=False, original = True)\n","rff_svm_linear.fit(x_train[indices], y_train[indices])\n","pred = rff_svm_linear.predict(x_test)\n","\n","accuracy_score(y_test, pred)"]},{"cell_type":"markdown","source":["**Ядровой SVM на случайных признаках**"],"metadata":{"id":"6wBW3WbvuLsQ"}},{"cell_type":"code","source":["%%time\n","rff_svm_kernel_pca = RFFPipeline(classifier = 'svm_kernel', use_PCA=True, original = False)\n","rff_svm_kernel_pca.fit(x_train[indices], y_train[indices])\n","pred = rff_svm_kernel_pca.predict(x_test)\n","accuracy_score(y_test, pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hOSxLsRguZko","executionInfo":{"status":"ok","timestamp":1708375030642,"user_tz":-180,"elapsed":117699,"user":{"displayName":"БИ Двестидвенадцать","userId":"11604785324055586986"}},"outputId":"b3c4fc53-8a6b-44a0-819c-be6227ba3fc9"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 1min 56s, sys: 2.68 s, total: 1min 59s\n","Wall time: 1min 57s\n"]},{"output_type":"execute_result","data":{"text/plain":["0.8539"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","source":["**Ядровой SVM на исходных признаках**"],"metadata":{"id":"Wyh1vPmXuaAV"}},{"cell_type":"code","source":["%%time\n","rff_svm_kernel = RFFPipeline(classifier = 'svm_kernel', use_PCA=False, original = True)\n","rff_svm_kernel.fit(x_train[indices], y_train[indices])\n","pred = rff_svm_kernel.predict(x_test)\n","\n","accuracy_score(y_test, pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-kjyWa2luhYQ","executionInfo":{"status":"ok","timestamp":1708375127203,"user_tz":-180,"elapsed":96569,"user":{"displayName":"БИ Двестидвенадцать","userId":"11604785324055586986"}},"outputId":"f2c149aa-8530-49d7-a61b-7f33ea476691"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 1min 34s, sys: 174 ms, total: 1min 35s\n","Wall time: 1min 35s\n"]},{"output_type":"execute_result","data":{"text/plain":["0.8647"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","source":["**Градиентный бустинг Catboost**\n","\n","Для градиентного бустинга создам здесь отдельно объект PCA"],"metadata":{"id":"NH4rOwyy4aQL"}},{"cell_type":"code","source":["!pip install catboost"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XavSyXFG4FNm","executionInfo":{"status":"ok","timestamp":1708355102416,"user_tz":-180,"elapsed":21835,"user":{"displayName":"Олег Данилин","userId":"14427680057428419926"}},"outputId":"ba2eebc7-1ef7-486d-ab84-94290c27ff72"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting catboost\n","  Downloading catboost-1.2.2-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.25.2)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.48.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n","Installing collected packages: catboost\n","Successfully installed catboost-1.2.2\n"]}]},{"cell_type":"code","source":["from catboost import CatBoostClassifier"],"metadata":{"id":"XFxcZxBl5d1L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Catboost с уменьшенной размерностью**"],"metadata":{"id":"VsISC7lNQv04"}},{"cell_type":"code","source":["%%time\n","boosting = CatBoostClassifier(num_trees=150, learning_rate=1e-1)\n","boost_rff = RFFPipeline()\n","X = boost_rff.transform(x_train[indices])\n","\n","boosting.fit(X, y_train[indices])\n","\n","\n","x = boost_rff.normalizer.transform(x_test)\n","x = boost_rff.red.transform(x)\n","pred = boosting.predict(x)\n","\n","accuracy_score(y_test, pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8sSElRio50up","executionInfo":{"status":"ok","timestamp":1708362293055,"user_tz":-180,"elapsed":36015,"user":{"displayName":"Олег Данилин","userId":"14427680057428419926"}},"outputId":"943de81a-96d0-418c-92aa-5f922a15736b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0:\tlearn: 1.9434913\ttotal: 615ms\tremaining: 1m 31s\n","1:\tlearn: 1.7237238\ttotal: 777ms\tremaining: 57.5s\n","2:\tlearn: 1.5870126\ttotal: 948ms\tremaining: 46.4s\n","3:\tlearn: 1.4639087\ttotal: 1.12s\tremaining: 40.8s\n","4:\tlearn: 1.3627576\ttotal: 1.31s\tremaining: 38s\n","5:\tlearn: 1.2796835\ttotal: 1.49s\tremaining: 35.8s\n","6:\tlearn: 1.2067467\ttotal: 1.66s\tremaining: 33.9s\n","7:\tlearn: 1.1456171\ttotal: 1.83s\tremaining: 32.5s\n","8:\tlearn: 1.0913310\ttotal: 2.01s\tremaining: 31.4s\n","9:\tlearn: 1.0466996\ttotal: 2.17s\tremaining: 30.4s\n","10:\tlearn: 1.0053994\ttotal: 2.36s\tremaining: 29.8s\n","11:\tlearn: 0.9677003\ttotal: 2.54s\tremaining: 29.2s\n","12:\tlearn: 0.9342792\ttotal: 2.7s\tremaining: 28.5s\n","13:\tlearn: 0.9035840\ttotal: 2.87s\tremaining: 27.9s\n","14:\tlearn: 0.8774520\ttotal: 3.06s\tremaining: 27.5s\n","15:\tlearn: 0.8559394\ttotal: 3.23s\tremaining: 27.1s\n","16:\tlearn: 0.8317836\ttotal: 3.45s\tremaining: 27s\n","17:\tlearn: 0.8135915\ttotal: 3.63s\tremaining: 26.7s\n","18:\tlearn: 0.7996039\ttotal: 3.81s\tremaining: 26.2s\n","19:\tlearn: 0.7816423\ttotal: 3.98s\tremaining: 25.9s\n","20:\tlearn: 0.7665337\ttotal: 4.15s\tremaining: 25.5s\n","21:\tlearn: 0.7516052\ttotal: 4.32s\tremaining: 25.1s\n","22:\tlearn: 0.7391916\ttotal: 4.51s\tremaining: 24.9s\n","23:\tlearn: 0.7250988\ttotal: 4.69s\tremaining: 24.6s\n","24:\tlearn: 0.7101454\ttotal: 4.86s\tremaining: 24.3s\n","25:\tlearn: 0.6998323\ttotal: 5.03s\tremaining: 24s\n","26:\tlearn: 0.6883908\ttotal: 5.2s\tremaining: 23.7s\n","27:\tlearn: 0.6778093\ttotal: 5.37s\tremaining: 23.4s\n","28:\tlearn: 0.6688896\ttotal: 5.55s\tremaining: 23.2s\n","29:\tlearn: 0.6602810\ttotal: 5.72s\tremaining: 22.9s\n","30:\tlearn: 0.6514502\ttotal: 5.9s\tremaining: 22.6s\n","31:\tlearn: 0.6441384\ttotal: 6.07s\tremaining: 22.4s\n","32:\tlearn: 0.6382353\ttotal: 6.24s\tremaining: 22.1s\n","33:\tlearn: 0.6290261\ttotal: 6.42s\tremaining: 21.9s\n","34:\tlearn: 0.6221943\ttotal: 6.61s\tremaining: 21.7s\n","35:\tlearn: 0.6145224\ttotal: 6.8s\tremaining: 21.5s\n","36:\tlearn: 0.6076408\ttotal: 6.97s\tremaining: 21.3s\n","37:\tlearn: 0.6020871\ttotal: 7.15s\tremaining: 21.1s\n","38:\tlearn: 0.5957902\ttotal: 7.32s\tremaining: 20.8s\n","39:\tlearn: 0.5900455\ttotal: 7.51s\tremaining: 20.6s\n","40:\tlearn: 0.5842328\ttotal: 7.68s\tremaining: 20.4s\n","41:\tlearn: 0.5782281\ttotal: 7.85s\tremaining: 20.2s\n","42:\tlearn: 0.5722313\ttotal: 8.03s\tremaining: 20s\n","43:\tlearn: 0.5652599\ttotal: 8.2s\tremaining: 19.7s\n","44:\tlearn: 0.5593988\ttotal: 8.37s\tremaining: 19.5s\n","45:\tlearn: 0.5551023\ttotal: 8.55s\tremaining: 19.3s\n","46:\tlearn: 0.5501108\ttotal: 8.73s\tremaining: 19.1s\n","47:\tlearn: 0.5443896\ttotal: 8.9s\tremaining: 18.9s\n","48:\tlearn: 0.5385400\ttotal: 9.07s\tremaining: 18.7s\n","49:\tlearn: 0.5333438\ttotal: 9.24s\tremaining: 18.5s\n","50:\tlearn: 0.5291581\ttotal: 9.42s\tremaining: 18.3s\n","51:\tlearn: 0.5247842\ttotal: 9.61s\tremaining: 18.1s\n","52:\tlearn: 0.5194051\ttotal: 9.78s\tremaining: 17.9s\n","53:\tlearn: 0.5149762\ttotal: 9.96s\tremaining: 17.7s\n","54:\tlearn: 0.5108408\ttotal: 10.1s\tremaining: 17.5s\n","55:\tlearn: 0.5073111\ttotal: 10.3s\tremaining: 17.3s\n","56:\tlearn: 0.5027691\ttotal: 10.5s\tremaining: 17.1s\n","57:\tlearn: 0.4985020\ttotal: 10.8s\tremaining: 17.1s\n","58:\tlearn: 0.4956232\ttotal: 11s\tremaining: 17s\n","59:\tlearn: 0.4921102\ttotal: 11.4s\tremaining: 17s\n","60:\tlearn: 0.4885925\ttotal: 11.7s\tremaining: 17.1s\n","61:\tlearn: 0.4850892\ttotal: 12s\tremaining: 17.1s\n","62:\tlearn: 0.4811357\ttotal: 12.3s\tremaining: 17s\n","63:\tlearn: 0.4780374\ttotal: 12.6s\tremaining: 17s\n","64:\tlearn: 0.4749853\ttotal: 12.9s\tremaining: 16.9s\n","65:\tlearn: 0.4726652\ttotal: 13.2s\tremaining: 16.8s\n","66:\tlearn: 0.4698363\ttotal: 13.5s\tremaining: 16.7s\n","67:\tlearn: 0.4667513\ttotal: 13.7s\tremaining: 16.5s\n","68:\tlearn: 0.4638236\ttotal: 13.8s\tremaining: 16.3s\n","69:\tlearn: 0.4608585\ttotal: 14s\tremaining: 16s\n","70:\tlearn: 0.4595446\ttotal: 14.2s\tremaining: 15.8s\n","71:\tlearn: 0.4562840\ttotal: 14.5s\tremaining: 15.7s\n","72:\tlearn: 0.4542902\ttotal: 14.7s\tremaining: 15.5s\n","73:\tlearn: 0.4519958\ttotal: 14.9s\tremaining: 15.3s\n","74:\tlearn: 0.4497227\ttotal: 15.1s\tremaining: 15.1s\n","75:\tlearn: 0.4478489\ttotal: 15.2s\tremaining: 14.8s\n","76:\tlearn: 0.4453000\ttotal: 15.4s\tremaining: 14.6s\n","77:\tlearn: 0.4422178\ttotal: 15.6s\tremaining: 14.4s\n","78:\tlearn: 0.4409937\ttotal: 15.8s\tremaining: 14.2s\n","79:\tlearn: 0.4394369\ttotal: 15.9s\tremaining: 14s\n","80:\tlearn: 0.4364734\ttotal: 16.1s\tremaining: 13.7s\n","81:\tlearn: 0.4338726\ttotal: 16.3s\tremaining: 13.5s\n","82:\tlearn: 0.4313877\ttotal: 16.5s\tremaining: 13.3s\n","83:\tlearn: 0.4299390\ttotal: 16.6s\tremaining: 13.1s\n","84:\tlearn: 0.4276717\ttotal: 16.8s\tremaining: 12.8s\n","85:\tlearn: 0.4253519\ttotal: 17s\tremaining: 12.6s\n","86:\tlearn: 0.4233698\ttotal: 17.2s\tremaining: 12.4s\n","87:\tlearn: 0.4209958\ttotal: 17.3s\tremaining: 12.2s\n","88:\tlearn: 0.4196641\ttotal: 17.5s\tremaining: 12s\n","89:\tlearn: 0.4181622\ttotal: 17.7s\tremaining: 11.8s\n","90:\tlearn: 0.4162003\ttotal: 17.8s\tremaining: 11.6s\n","91:\tlearn: 0.4149142\ttotal: 18s\tremaining: 11.4s\n","92:\tlearn: 0.4125388\ttotal: 18.2s\tremaining: 11.2s\n","93:\tlearn: 0.4106317\ttotal: 18.4s\tremaining: 10.9s\n","94:\tlearn: 0.4087819\ttotal: 18.5s\tremaining: 10.7s\n","95:\tlearn: 0.4062082\ttotal: 18.7s\tremaining: 10.5s\n","96:\tlearn: 0.4048716\ttotal: 18.9s\tremaining: 10.3s\n","97:\tlearn: 0.4028729\ttotal: 19.1s\tremaining: 10.1s\n","98:\tlearn: 0.4011453\ttotal: 19.2s\tremaining: 9.91s\n","99:\tlearn: 0.3993601\ttotal: 19.4s\tremaining: 9.7s\n","100:\tlearn: 0.3977561\ttotal: 19.6s\tremaining: 9.5s\n","101:\tlearn: 0.3960388\ttotal: 19.7s\tremaining: 9.29s\n","102:\tlearn: 0.3936677\ttotal: 19.9s\tremaining: 9.09s\n","103:\tlearn: 0.3923912\ttotal: 20.1s\tremaining: 8.89s\n","104:\tlearn: 0.3910738\ttotal: 20.3s\tremaining: 8.69s\n","105:\tlearn: 0.3898312\ttotal: 20.5s\tremaining: 8.49s\n","106:\tlearn: 0.3881514\ttotal: 20.6s\tremaining: 8.29s\n","107:\tlearn: 0.3863173\ttotal: 20.8s\tremaining: 8.09s\n","108:\tlearn: 0.3849348\ttotal: 21s\tremaining: 7.89s\n","109:\tlearn: 0.3836702\ttotal: 21.2s\tremaining: 7.69s\n","110:\tlearn: 0.3827679\ttotal: 21.3s\tremaining: 7.49s\n","111:\tlearn: 0.3809051\ttotal: 21.5s\tremaining: 7.29s\n","112:\tlearn: 0.3796830\ttotal: 21.7s\tremaining: 7.09s\n","113:\tlearn: 0.3783402\ttotal: 21.8s\tremaining: 6.89s\n","114:\tlearn: 0.3774339\ttotal: 22s\tremaining: 6.69s\n","115:\tlearn: 0.3762293\ttotal: 22.2s\tremaining: 6.5s\n","116:\tlearn: 0.3753687\ttotal: 22.4s\tremaining: 6.3s\n","117:\tlearn: 0.3744290\ttotal: 22.5s\tremaining: 6.11s\n","118:\tlearn: 0.3727770\ttotal: 22.7s\tremaining: 5.91s\n","119:\tlearn: 0.3714980\ttotal: 22.9s\tremaining: 5.72s\n","120:\tlearn: 0.3702287\ttotal: 23s\tremaining: 5.52s\n","121:\tlearn: 0.3693993\ttotal: 23.2s\tremaining: 5.33s\n","122:\tlearn: 0.3674851\ttotal: 23.4s\tremaining: 5.14s\n","123:\tlearn: 0.3662745\ttotal: 23.7s\tremaining: 4.97s\n","124:\tlearn: 0.3655469\ttotal: 24s\tremaining: 4.81s\n","125:\tlearn: 0.3647894\ttotal: 24.4s\tremaining: 4.64s\n","126:\tlearn: 0.3637632\ttotal: 24.7s\tremaining: 4.47s\n","127:\tlearn: 0.3624126\ttotal: 25s\tremaining: 4.29s\n","128:\tlearn: 0.3618283\ttotal: 25.3s\tremaining: 4.12s\n","129:\tlearn: 0.3606594\ttotal: 25.6s\tremaining: 3.94s\n","130:\tlearn: 0.3594346\ttotal: 25.9s\tremaining: 3.76s\n","131:\tlearn: 0.3577664\ttotal: 26.1s\tremaining: 3.56s\n","132:\tlearn: 0.3569100\ttotal: 26.3s\tremaining: 3.36s\n","133:\tlearn: 0.3562261\ttotal: 26.5s\tremaining: 3.16s\n","134:\tlearn: 0.3549578\ttotal: 26.6s\tremaining: 2.96s\n","135:\tlearn: 0.3535093\ttotal: 26.8s\tremaining: 2.76s\n","136:\tlearn: 0.3519992\ttotal: 27s\tremaining: 2.56s\n","137:\tlearn: 0.3508066\ttotal: 27.2s\tremaining: 2.36s\n","138:\tlearn: 0.3502454\ttotal: 27.3s\tremaining: 2.16s\n","139:\tlearn: 0.3485170\ttotal: 27.5s\tremaining: 1.97s\n","140:\tlearn: 0.3475372\ttotal: 27.7s\tremaining: 1.77s\n","141:\tlearn: 0.3464435\ttotal: 27.9s\tremaining: 1.57s\n","142:\tlearn: 0.3455799\ttotal: 28s\tremaining: 1.37s\n","143:\tlearn: 0.3443190\ttotal: 28.2s\tremaining: 1.18s\n","144:\tlearn: 0.3434123\ttotal: 28.4s\tremaining: 979ms\n","145:\tlearn: 0.3426319\ttotal: 28.6s\tremaining: 782ms\n","146:\tlearn: 0.3413932\ttotal: 28.7s\tremaining: 586ms\n","147:\tlearn: 0.3405030\ttotal: 28.9s\tremaining: 390ms\n","148:\tlearn: 0.3390211\ttotal: 29.1s\tremaining: 195ms\n","149:\tlearn: 0.3381506\ttotal: 29.2s\tremaining: 0us\n","CPU times: user 54.2 s, sys: 1.62 s, total: 55.8 s\n","Wall time: 35.8 s\n"]},{"output_type":"execute_result","data":{"text/plain":["0.8331"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","source":["**Catboost с исходной размерностью**\n"],"metadata":{"id":"1-4jx8b7Q84t"}},{"cell_type":"code","source":["%%time\n","boosting = CatBoostClassifier(num_trees=150, learning_rate=1e-1)\n","norm = Normalizer()\n","norm.fit(x_train[indices])\n","X = norm.transform(x_train[indices])\n","boosting.fit(X, y_train[indices])\n","pred = boosting.predict(norm.transform(x_test))\n","\n","accuracy_score(y_test, pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DCXTCj-iFR6D","executionInfo":{"status":"ok","timestamp":1708362691417,"user_tz":-180,"elapsed":398377,"user":{"displayName":"Олег Данилин","userId":"14427680057428419926"}},"outputId":"b0a787eb-e6e6-4a27-86c8-64eb74cbfd72"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0:\tlearn: 1.9256791\ttotal: 3s\tremaining: 7m 27s\n","1:\tlearn: 1.7010462\ttotal: 6.07s\tremaining: 7m 29s\n","2:\tlearn: 1.5360769\ttotal: 8.57s\tremaining: 6m 59s\n","3:\tlearn: 1.4144683\ttotal: 11.1s\tremaining: 6m 44s\n","4:\tlearn: 1.3168790\ttotal: 13.6s\tremaining: 6m 34s\n","5:\tlearn: 1.2335023\ttotal: 17s\tremaining: 6m 48s\n","6:\tlearn: 1.1606865\ttotal: 19.8s\tremaining: 6m 44s\n","7:\tlearn: 1.0956322\ttotal: 22.3s\tremaining: 6m 36s\n","8:\tlearn: 1.0393231\ttotal: 24.8s\tremaining: 6m 29s\n","9:\tlearn: 0.9918201\ttotal: 27.3s\tremaining: 6m 22s\n","10:\tlearn: 0.9488349\ttotal: 30.9s\tremaining: 6m 30s\n","11:\tlearn: 0.9084119\ttotal: 33.3s\tremaining: 6m 22s\n","12:\tlearn: 0.8743001\ttotal: 35.7s\tremaining: 6m 16s\n","13:\tlearn: 0.8436743\ttotal: 38.2s\tremaining: 6m 10s\n","14:\tlearn: 0.8177970\ttotal: 40.5s\tremaining: 6m 4s\n","15:\tlearn: 0.7906318\ttotal: 44.1s\tremaining: 6m 9s\n","16:\tlearn: 0.7685729\ttotal: 46.4s\tremaining: 6m 3s\n","17:\tlearn: 0.7451341\ttotal: 48.8s\tremaining: 5m 57s\n","18:\tlearn: 0.7233620\ttotal: 51.1s\tremaining: 5m 52s\n","19:\tlearn: 0.7028776\ttotal: 53.5s\tremaining: 5m 47s\n","20:\tlearn: 0.6851055\ttotal: 57s\tremaining: 5m 50s\n","21:\tlearn: 0.6691048\ttotal: 59.4s\tremaining: 5m 45s\n","22:\tlearn: 0.6578484\ttotal: 1m 1s\tremaining: 5m 40s\n","23:\tlearn: 0.6453426\ttotal: 1m 4s\tremaining: 5m 36s\n","24:\tlearn: 0.6324001\ttotal: 1m 6s\tremaining: 5m 32s\n","25:\tlearn: 0.6219183\ttotal: 1m 9s\tremaining: 5m 33s\n","26:\tlearn: 0.6107572\ttotal: 1m 12s\tremaining: 5m 29s\n","27:\tlearn: 0.5991708\ttotal: 1m 14s\tremaining: 5m 25s\n","28:\tlearn: 0.5897665\ttotal: 1m 17s\tremaining: 5m 21s\n","29:\tlearn: 0.5806631\ttotal: 1m 19s\tremaining: 5m 17s\n","30:\tlearn: 0.5741178\ttotal: 1m 22s\tremaining: 5m 18s\n","31:\tlearn: 0.5669567\ttotal: 1m 25s\tremaining: 5m 14s\n","32:\tlearn: 0.5593900\ttotal: 1m 27s\tremaining: 5m 10s\n","33:\tlearn: 0.5498968\ttotal: 1m 29s\tremaining: 5m 7s\n","34:\tlearn: 0.5444662\ttotal: 1m 32s\tremaining: 5m 3s\n","35:\tlearn: 0.5383364\ttotal: 1m 35s\tremaining: 5m 3s\n","36:\tlearn: 0.5306309\ttotal: 1m 38s\tremaining: 4m 59s\n","37:\tlearn: 0.5247241\ttotal: 1m 40s\tremaining: 4m 56s\n","38:\tlearn: 0.5188656\ttotal: 1m 42s\tremaining: 4m 52s\n","39:\tlearn: 0.5140796\ttotal: 1m 45s\tremaining: 4m 49s\n","40:\tlearn: 0.5073559\ttotal: 1m 48s\tremaining: 4m 49s\n","41:\tlearn: 0.5026655\ttotal: 1m 51s\tremaining: 4m 45s\n","42:\tlearn: 0.4980266\ttotal: 1m 53s\tremaining: 4m 42s\n","43:\tlearn: 0.4931283\ttotal: 1m 55s\tremaining: 4m 39s\n","44:\tlearn: 0.4879281\ttotal: 1m 58s\tremaining: 4m 35s\n","45:\tlearn: 0.4832522\ttotal: 2m 1s\tremaining: 4m 35s\n","46:\tlearn: 0.4791304\ttotal: 2m 4s\tremaining: 4m 31s\n","47:\tlearn: 0.4743022\ttotal: 2m 6s\tremaining: 4m 28s\n","48:\tlearn: 0.4704284\ttotal: 2m 8s\tremaining: 4m 25s\n","49:\tlearn: 0.4671579\ttotal: 2m 11s\tremaining: 4m 22s\n","50:\tlearn: 0.4627731\ttotal: 2m 14s\tremaining: 4m 21s\n","51:\tlearn: 0.4597169\ttotal: 2m 17s\tremaining: 4m 18s\n","52:\tlearn: 0.4567481\ttotal: 2m 19s\tremaining: 4m 15s\n","53:\tlearn: 0.4521295\ttotal: 2m 21s\tremaining: 4m 12s\n","54:\tlearn: 0.4488688\ttotal: 2m 24s\tremaining: 4m 8s\n","55:\tlearn: 0.4455330\ttotal: 2m 27s\tremaining: 4m 7s\n","56:\tlearn: 0.4423114\ttotal: 2m 30s\tremaining: 4m 4s\n","57:\tlearn: 0.4389827\ttotal: 2m 32s\tremaining: 4m 1s\n","58:\tlearn: 0.4362202\ttotal: 2m 34s\tremaining: 3m 58s\n","59:\tlearn: 0.4334161\ttotal: 2m 37s\tremaining: 3m 55s\n","60:\tlearn: 0.4308782\ttotal: 2m 40s\tremaining: 3m 54s\n","61:\tlearn: 0.4285465\ttotal: 2m 42s\tremaining: 3m 51s\n","62:\tlearn: 0.4247111\ttotal: 2m 45s\tremaining: 3m 48s\n","63:\tlearn: 0.4220594\ttotal: 2m 47s\tremaining: 3m 45s\n","64:\tlearn: 0.4188680\ttotal: 2m 50s\tremaining: 3m 42s\n","65:\tlearn: 0.4160669\ttotal: 2m 53s\tremaining: 3m 40s\n","66:\tlearn: 0.4139204\ttotal: 2m 55s\tremaining: 3m 37s\n","67:\tlearn: 0.4117910\ttotal: 2m 58s\tremaining: 3m 34s\n","68:\tlearn: 0.4092966\ttotal: 3m\tremaining: 3m 32s\n","69:\tlearn: 0.4070088\ttotal: 3m 2s\tremaining: 3m 29s\n","70:\tlearn: 0.4046093\ttotal: 3m 6s\tremaining: 3m 27s\n","71:\tlearn: 0.4026978\ttotal: 3m 8s\tremaining: 3m 24s\n","72:\tlearn: 0.4003755\ttotal: 3m 11s\tremaining: 3m 21s\n","73:\tlearn: 0.3978550\ttotal: 3m 13s\tremaining: 3m 18s\n","74:\tlearn: 0.3959672\ttotal: 3m 15s\tremaining: 3m 15s\n","75:\tlearn: 0.3937898\ttotal: 3m 19s\tremaining: 3m 14s\n","76:\tlearn: 0.3911690\ttotal: 3m 21s\tremaining: 3m 11s\n","77:\tlearn: 0.3894757\ttotal: 3m 24s\tremaining: 3m 8s\n","78:\tlearn: 0.3877575\ttotal: 3m 26s\tremaining: 3m 5s\n","79:\tlearn: 0.3862527\ttotal: 3m 28s\tremaining: 3m 2s\n","80:\tlearn: 0.3844271\ttotal: 3m 32s\tremaining: 3m\n","81:\tlearn: 0.3829166\ttotal: 3m 34s\tremaining: 2m 58s\n","82:\tlearn: 0.3818056\ttotal: 3m 37s\tremaining: 2m 55s\n","83:\tlearn: 0.3803903\ttotal: 3m 39s\tremaining: 2m 52s\n","84:\tlearn: 0.3787030\ttotal: 3m 41s\tremaining: 2m 49s\n","85:\tlearn: 0.3769313\ttotal: 3m 45s\tremaining: 2m 47s\n","86:\tlearn: 0.3755880\ttotal: 3m 47s\tremaining: 2m 44s\n","87:\tlearn: 0.3731019\ttotal: 3m 50s\tremaining: 2m 42s\n","88:\tlearn: 0.3710639\ttotal: 3m 52s\tremaining: 2m 39s\n","89:\tlearn: 0.3697277\ttotal: 3m 54s\tremaining: 2m 36s\n","90:\tlearn: 0.3671473\ttotal: 3m 58s\tremaining: 2m 34s\n","91:\tlearn: 0.3652578\ttotal: 4m\tremaining: 2m 31s\n","92:\tlearn: 0.3635026\ttotal: 4m 2s\tremaining: 2m 28s\n","93:\tlearn: 0.3620658\ttotal: 4m 5s\tremaining: 2m 26s\n","94:\tlearn: 0.3608747\ttotal: 4m 7s\tremaining: 2m 23s\n","95:\tlearn: 0.3596993\ttotal: 4m 11s\tremaining: 2m 21s\n","96:\tlearn: 0.3586969\ttotal: 4m 13s\tremaining: 2m 18s\n","97:\tlearn: 0.3569834\ttotal: 4m 15s\tremaining: 2m 15s\n","98:\tlearn: 0.3560100\ttotal: 4m 18s\tremaining: 2m 13s\n","99:\tlearn: 0.3544515\ttotal: 4m 20s\tremaining: 2m 10s\n","100:\tlearn: 0.3532324\ttotal: 4m 24s\tremaining: 2m 8s\n","101:\tlearn: 0.3523268\ttotal: 4m 26s\tremaining: 2m 5s\n","102:\tlearn: 0.3513314\ttotal: 4m 28s\tremaining: 2m 2s\n","103:\tlearn: 0.3505850\ttotal: 4m 31s\tremaining: 1m 59s\n","104:\tlearn: 0.3492155\ttotal: 4m 33s\tremaining: 1m 57s\n","105:\tlearn: 0.3475461\ttotal: 4m 37s\tremaining: 1m 55s\n","106:\tlearn: 0.3463767\ttotal: 4m 39s\tremaining: 1m 52s\n","107:\tlearn: 0.3453867\ttotal: 4m 41s\tremaining: 1m 49s\n","108:\tlearn: 0.3441143\ttotal: 4m 44s\tremaining: 1m 46s\n","109:\tlearn: 0.3427415\ttotal: 4m 46s\tremaining: 1m 44s\n","110:\tlearn: 0.3418410\ttotal: 4m 50s\tremaining: 1m 41s\n","111:\tlearn: 0.3405880\ttotal: 4m 52s\tremaining: 1m 39s\n","112:\tlearn: 0.3397148\ttotal: 4m 54s\tremaining: 1m 36s\n","113:\tlearn: 0.3390408\ttotal: 4m 57s\tremaining: 1m 33s\n","114:\tlearn: 0.3379107\ttotal: 4m 59s\tremaining: 1m 31s\n","115:\tlearn: 0.3372339\ttotal: 5m 2s\tremaining: 1m 28s\n","116:\tlearn: 0.3360398\ttotal: 5m 5s\tremaining: 1m 26s\n","117:\tlearn: 0.3348092\ttotal: 5m 7s\tremaining: 1m 23s\n","118:\tlearn: 0.3335812\ttotal: 5m 9s\tremaining: 1m 20s\n","119:\tlearn: 0.3331839\ttotal: 5m 12s\tremaining: 1m 18s\n","120:\tlearn: 0.3322273\ttotal: 5m 15s\tremaining: 1m 15s\n","121:\tlearn: 0.3312709\ttotal: 5m 18s\tremaining: 1m 13s\n","122:\tlearn: 0.3300432\ttotal: 5m 20s\tremaining: 1m 10s\n","123:\tlearn: 0.3294143\ttotal: 5m 22s\tremaining: 1m 7s\n","124:\tlearn: 0.3283365\ttotal: 5m 25s\tremaining: 1m 5s\n","125:\tlearn: 0.3272461\ttotal: 5m 28s\tremaining: 1m 2s\n","126:\tlearn: 0.3260118\ttotal: 5m 31s\tremaining: 60s\n","127:\tlearn: 0.3254800\ttotal: 5m 33s\tremaining: 57.3s\n","128:\tlearn: 0.3250567\ttotal: 5m 35s\tremaining: 54.7s\n","129:\tlearn: 0.3240926\ttotal: 5m 38s\tremaining: 52s\n","130:\tlearn: 0.3226516\ttotal: 5m 41s\tremaining: 49.6s\n","131:\tlearn: 0.3220112\ttotal: 5m 44s\tremaining: 46.9s\n","132:\tlearn: 0.3213700\ttotal: 5m 46s\tremaining: 44.3s\n","133:\tlearn: 0.3205396\ttotal: 5m 48s\tremaining: 41.6s\n","134:\tlearn: 0.3197389\ttotal: 5m 51s\tremaining: 39s\n","135:\tlearn: 0.3188884\ttotal: 5m 54s\tremaining: 36.5s\n","136:\tlearn: 0.3180867\ttotal: 5m 56s\tremaining: 33.9s\n","137:\tlearn: 0.3174410\ttotal: 5m 59s\tremaining: 31.2s\n","138:\tlearn: 0.3165625\ttotal: 6m 1s\tremaining: 28.6s\n","139:\tlearn: 0.3158881\ttotal: 6m 4s\tremaining: 26s\n","140:\tlearn: 0.3151837\ttotal: 6m 7s\tremaining: 23.5s\n","141:\tlearn: 0.3143286\ttotal: 6m 9s\tremaining: 20.8s\n","142:\tlearn: 0.3138054\ttotal: 6m 12s\tremaining: 18.2s\n","143:\tlearn: 0.3128371\ttotal: 6m 14s\tremaining: 15.6s\n","144:\tlearn: 0.3118880\ttotal: 6m 17s\tremaining: 13s\n","145:\tlearn: 0.3116117\ttotal: 6m 20s\tremaining: 10.4s\n","146:\tlearn: 0.3109143\ttotal: 6m 22s\tremaining: 7.81s\n","147:\tlearn: 0.3093720\ttotal: 6m 25s\tremaining: 5.2s\n","148:\tlearn: 0.3087190\ttotal: 6m 27s\tremaining: 2.6s\n","149:\tlearn: 0.3082033\ttotal: 6m 30s\tremaining: 0us\n","CPU times: user 11min 39s, sys: 1.07 s, total: 11min 40s\n","Wall time: 6min 38s\n"]},{"output_type":"execute_result","data":{"text/plain":["0.8496"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","source":["\n","Linreg и SVM (linear) показали лучшее качество со случайными признаками. Также Linreg со случайными признаками обучается гораздо быстрее.\n","\n","Бустинг показал качество немного получше на исходной выборке, но за гораздо большее время обучения. Вероятно, это связано с тем, что бустинг отслеживает далеко не линейные зависимости, и ему не нужен метод главных компонент.\n","\n","Идея со случайными признаками может улучшить обучение как с точки зрения качества, так и с точки зрения скорости. Главное - правильно подобранные гиперпараметры (включая саму модель)"],"metadata":{"id":"SXikZUCz2wdJ"}},{"cell_type":"markdown","metadata":{"id":"e6umjhWuK-hV"},"source":["__Задание 3. (2 балла)__\n","\n","Проведите эксперименты:\n","1. Помогает ли предварительное понижение размерности с помощью PCA?\n","2. Как зависит итоговое качество от n_features? Выходит ли оно на плато при росте n_features?\n","3. Важно ли, какую модель обучать — логистическую регрессию или SVM?"]},{"cell_type":"markdown","source":["**1. Помогает ли предварительное понижение размерности с помощью PCA?**"],"metadata":{"id":"JeYL0b1sH89p"}},{"cell_type":"markdown","source":["Вопрос действительно на подумать.\n","\n","Заметно меньше по времени обучается модель при низкой размерности независимо от моделей.\n","\n","Без PCA даже не получалось запустить полную выборку в rffpipeline при гиперпараметрах по умолчаниию, исчерпывался весь ОЗУ.\n","\n","Блоком выше решил запустить бустинг на подвыборке с размерностью 784, а не 50. Обучение гораздо дольше, по качеству выше на одну сотую.\n","\n","В качестве эксперимента можно сравнить обучение на подвыборках с pca и без:"],"metadata":{"id":"LCpJoQGwGbpM"}},{"cell_type":"code","execution_count":11,"metadata":{"id":"c2QIHIMbK-hW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708368619981,"user_tz":-180,"elapsed":6851,"user":{"displayName":"БИ Двестидвенадцать","userId":"11604785324055586986"}},"outputId":"652c6a28-5fe5-486d-cd98-15f6363c1c2e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 10.1 s, sys: 1.34 s, total: 11.5 s\n","Wall time: 6.6 s\n"]},{"output_type":"execute_result","data":{"text/plain":["0.844"]},"metadata":{},"execution_count":11}],"source":["%%time\n","rff_logreg = RFFPipeline(use_PCA = False, new_dim = x_train.shape[1], n_features = 500)\n","rff_logreg.fit(x_train[indices], y_train[indices])\n","pred = rff_logreg.predict(x_test)\n","accuracy_score(y_test, pred)"]},{"cell_type":"code","source":["%%time\n","rff_logreg = RFFPipeline(n_features = 500)\n","rff_logreg.fit(x_train[indices], y_train[indices])\n","pred = rff_logreg.predict(x_test)\n","accuracy_score(y_test, pred)"],"metadata":{"id":"scrwX1-7bGiG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708368649615,"user_tz":-180,"elapsed":9383,"user":{"displayName":"БИ Двестидвенадцать","userId":"11604785324055586986"}},"outputId":"535c1bad-a5f3-48bc-e8aa-e45e1552c317"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 12.8 s, sys: 3.42 s, total: 16.2 s\n","Wall time: 8.8 s\n"]},{"output_type":"execute_result","data":{"text/plain":["0.8523"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["С PCA accuracy на одну сотую больше"],"metadata":{"id":"9LrFhbXztbfW"}},{"cell_type":"markdown","source":["**2. Как зависит итоговое качество от n_features? Выходит ли оно на плато при росте n_features?**"],"metadata":{"id":"1ZqR8J9pKBLi"}},{"cell_type":"code","source":["%%time\n","features = [100, 500, 1000, 2000, 2500, 5000]\n","acc_lin = []\n","for f in features:\n","  rff_logreg = RFFPipeline(n_features=f)\n","  rff_logreg.fit(x_train[indices], y_train[indices])\n","  pred = rff_logreg.predict(x_test)\n","  acc_lin.append(accuracy_score(y_test, pred))"],"metadata":{"id":"SUrnWDGSKDcD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708369432194,"user_tz":-180,"elapsed":168074,"user":{"displayName":"БИ Двестидвенадцать","userId":"11604785324055586986"}},"outputId":"da5e1007-afc9-410e-d65d-4545208b9d0e"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]}]},{"cell_type":"code","source":["plt.plot(features, acc_lin)\n","plt.ylabel('accuracy')\n","plt.xlabel('n_features')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467},"id":"jJ1V3XjsuwjM","executionInfo":{"status":"ok","timestamp":1708370026737,"user_tz":-180,"elapsed":535,"user":{"displayName":"БИ Двестидвенадцать","userId":"11604785324055586986"}},"outputId":"357f426b-a3eb-4f0d-9c3e-8dbe9e0a2372"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 0, 'n_features')"]},"metadata":{},"execution_count":19},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["Примерно до 2800 качество растет до 0.86, затем, начиная с 3000, выходит на плато"],"metadata":{"id":"w5BQSTdK2FqH"}},{"cell_type":"markdown","source":["**Важно ли, какую модель обучать — логистическую регрессию или SVM?**"],"metadata":{"id":"sjOnl22YvF3l"}},{"cell_type":"code","source":["%%time\n","features = [100, 500, 1000, 2000, 2500, 5000]\n","acc_svm_linear = []\n","for f in features:\n","  rff_svm = RFFPipeline(n_features=f, classifier='svm_linear')\n","  rff_svm.fit(x_train[indices], y_train[indices])\n","  pred = rff_svm.predict(x_test)\n","  acc_svm_linear.append(accuracy_score(y_test, pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ync0QMVQzgMN","executionInfo":{"status":"ok","timestamp":1708371838249,"user_tz":-180,"elapsed":962392,"user":{"displayName":"БИ Двестидвенадцать","userId":"11604785324055586986"}},"outputId":"e002b175-28fe-4179-e429-e98bd3ad2d28"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 15min 15s, sys: 17.3 s, total: 15min 32s\n","Wall time: 16min 2s\n"]}]},{"cell_type":"code","source":["acc_lin > acc_svm_linear"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"656hiv2X5x8Y","executionInfo":{"status":"ok","timestamp":1708372031259,"user_tz":-180,"elapsed":341,"user":{"displayName":"БИ Двестидвенадцать","userId":"11604785324055586986"}},"outputId":"c1f46cc6-ffa8-4820-8f93-66c57851e2a7"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["acc_lin"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xg2yYVhj51x5","executionInfo":{"status":"ok","timestamp":1708372146730,"user_tz":-180,"elapsed":252,"user":{"displayName":"БИ Двестидвенадцать","userId":"11604785324055586986"}},"outputId":"54c12d55-fd4f-42a2-f935-afeb3b7d404a"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.8325, 0.8521, 0.8563, 0.8577, 0.8598, 0.8601]"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["acc_svm_linear"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j_3lgvN-6kMv","executionInfo":{"status":"ok","timestamp":1708372152138,"user_tz":-180,"elapsed":5,"user":{"displayName":"БИ Двестидвенадцать","userId":"11604785324055586986"}},"outputId":"3e2f2fe3-4aee-431e-dd83-3c5b3f1bbafd"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.8313, 0.8469, 0.8503, 0.8485, 0.8521, 0.8531]"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["Лучше себя показала логистическая регрессия, обучилась гораздло быстрее, и с лучшим качеством при разных значениях n_features"],"metadata":{"id":"q7x5vLmWvNSh"}},{"cell_type":"markdown","metadata":{"id":"CJqXVuasK-hW"},"source":["### Бонус"]},{"cell_type":"markdown","metadata":{"id":"QVDWHCdrK-hX"},"source":["__Задание 4. (Максимум 2 балла)__\n","\n","Как вы, должно быть, помните с курса МО-1, многие алгоритмы машинного обучения работают лучше, если признаки данных некоррелированы. Оказывается, что для RFF существует модификация, позволяющая получать ортогональные случайные признаки (Orthogonal Random Features, ORF). Об этом методе можно прочитать в [статье](https://proceedings.neurips.cc/paper/2016/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf). Реализуйте класс для вычисления ORF по аналогии с основным заданием. Обратите внимание, что ваш класс должен уметь работать со случаем n_features > new_dim (в статье есть замечание на этот счет). Проведите эксперименты, сравнивающие RFF и ORF, сделайте выводы."]},{"cell_type":"code","execution_count":41,"metadata":{"id":"HSxvGI9iK-hX","executionInfo":{"status":"ok","timestamp":1708374641749,"user_tz":-180,"elapsed":270,"user":{"displayName":"БИ Двестидвенадцать","userId":"11604785324055586986"}}},"outputs":[],"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.linear_model import LogisticRegression\n","\n","from sklearn import svm\n","from sklearn.preprocessing import StandardScaler, Normalizer\n","\n","\n","\n","\n","class ORFPipeline(BaseEstimator, TransformerMixin):\n","    def __init__(self, n_features=1000, new_dim=50, use_PCA=True, classifier='logreg', original = False):\n","        \"\"\"\n","        Implements pipeline, which consists of PCA decomposition,\n","        Random Fourier Features approximation and linear classification model.\n","\n","        n_features, int: amount of synthetic random features generated with RFF approximation.\n","\n","        new_dim, int: PCA output size.\n","\n","        use_PCA, bool: whether to include PCA preprocessing.\n","\n","        classifier, string: either 'svm' or 'logreg', a linear classification model to use on top of pipeline.\n","\n","        Feel free to edit this template for your preferences.\n","        \"\"\"\n","        self.n_features = n_features\n","        self.use_PCA = use_PCA\n","        self.new_dim = new_dim\n","        self.classifier = classifier\n","        self.model = LogisticRegression()\n","        self.red = PCA(n_components = new_dim)\n","        self.normalizer = Normalizer()\n","        self.original = original\n","        self.w = None\n","        self.b = None\n","\n","        if self.classifier == 'svm_linear':\n","          self.model = svm.LinearSVC()\n","        elif self.classifier == 'svm_kernel':\n","          self.model = svm.SVC(kernel='rbf')\n","\n","\n","\n","\n","\n","    def transform(self, X):\n","      self.normalizer.fit(X)\n","      X = self.normalizer.transform(X)\n","      self.red.fit(X)\n","      X = self.red.transform(X)\n","\n","      return X\n","\n","    def fit(self, X, y):\n","        \"\"\"\n","        Fit all parts of algorithm (PCA, RFF, Classification) to training set.\n","        \"\"\"\n","        self.normalizer.fit(X)\n","        X = self.normalizer.transform(X)\n","\n","        if self.use_PCA:\n","          self.red.fit(X)\n","          X = self.red.transform(X)\n","\n","        # генерируем миллион пар и оцениваем сигму\n","        if self.original == False:\n","          indices_1 = np.random.choice(X.shape[0], int(1000100 / (1 + self.new_dim - 50))) # мы уменьшаем количество сэмплов, если new_dim будет больше 50\n","          indices_2 = np.random.choice(X.shape[0], int(1000100 / (1 + self.new_dim - 50)))\n","          result = np.sum((X[indices_1] - X[indices_2]) ** 2, axis = 1)\n","          sigma = np.median(result[result != 0])\n","\n","          self.w = np.random.normal(0, 1 / sigma, size = (self.n_features, self.new_dim))\n","          q, r = np.linalg.qr(self.w)\n","          s_elems = np.random.chisquare(self.n_features, size=(self.new_dim, 1))\n","          print(s_elems)\n","          s = np.diag(s_elems, k=s_elems.shape[0])\n","\n","          print(s.shape)\n","          print(q.shape)\n","\n","          self.w = (1 / sigma) * s @ q\n","          self.b = np.random.uniform(-np.pi, np.pi, self.n_features)\n","\n","          X = np.cos(X @ self.w.T + self.b)\n","\n","        self.model.fit(X, y)\n","\n","        return self\n","\n","    def predict_proba(self, X):\n","        \"\"\"\n","        Apply pipeline to obtain scores for input data.\n","        \"\"\"\n","        raise NotImplementedError\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Apply pipeline to obtain discrete predictions for input data.\n","        \"\"\"\n","        X = self.normalizer.transform(X)\n","        if self.use_PCA:\n","          X = self.red.transform(X)\n","        if self.original == False:\n","          X = np.cos(X @ self.w.T + self.b)\n","\n","        return self.model.predict(X)"]},{"cell_type":"code","source":["# протестируем на logreg\n","%%time\n","orf_logreg = ORFPipeline()\n","orf_logreg.fit(x_train, y_train)\n","pred_orf = rff_logreg.predict(x_test)\n","accuracy_score(y_test, pred_orf)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"lg2ITI31AA4y","executionInfo":{"status":"ok","timestamp":1708374654328,"user_tz":-180,"elapsed":9950,"user":{"displayName":"БИ Двестидвенадцать","userId":"11604785324055586986"}},"outputId":"13f1a62e-5869-4803-c5aa-3fa52b93baf5"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1015.01492577]\n"," [ 895.41276149]\n"," [1076.06994302]\n"," [1027.4961952 ]\n"," [ 960.52472442]\n"," [1030.12637485]\n"," [ 986.78367808]\n"," [1051.04834199]\n"," [1042.78155658]\n"," [ 939.51740765]\n"," [ 999.35317385]\n"," [ 960.50374047]\n"," [1049.15398275]\n"," [ 927.12218567]\n"," [ 981.67453636]\n"," [1011.62418364]\n"," [ 968.8917735 ]\n"," [1000.24858908]\n"," [ 996.73743985]\n"," [1038.62520647]\n"," [ 981.95410234]\n"," [ 993.43746902]\n"," [ 954.91495785]\n"," [ 934.70803687]\n"," [ 944.69866678]\n"," [ 992.4179424 ]\n"," [ 935.91081293]\n"," [ 987.91495718]\n"," [ 993.68604366]\n"," [1045.22174463]\n"," [1012.3028481 ]\n"," [ 957.64374163]\n"," [1000.08622021]\n"," [1004.33697237]\n"," [ 947.61708499]\n"," [1017.56376969]\n"," [1004.18228084]\n"," [ 975.76363702]\n"," [1038.80989079]\n"," [1026.58953309]\n"," [ 917.46819119]\n"," [1029.57956966]\n"," [1098.54221102]\n"," [1051.41784528]\n"," [1020.63953908]\n"," [1019.11455761]\n"," [ 993.33523195]\n"," [ 979.66600479]\n"," [1038.38792934]\n"," [1028.7807024 ]]\n","(0,)\n","(1000, 50)\n"]},{"output_type":"error","ename":"ValueError","evalue":"matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1000 is different from 0)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[0;32m<ipython-input-41-bde8b081477a>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     79\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1000 is different from 0)"]}]},{"cell_type":"markdown","metadata":{"id":"4pc7-1jmK-hY"},"source":["__Задание 5. (Максимум 1 балл)__\n","\n","Поэкспериментируйте с функциями для вычисления новых случайных признаков. Не обязательно использовать косинус от скалярного произведения — можно брать знак от него, хэш и т.д. Придумайте побольше вариантов для генерации признаков и проверьте, не получается ли с их помощью добиваться более высокого качества. Также можете попробовать другой классификатор поверх случайных признаков, сравните результаты."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dWj-O2vjK-hY"},"outputs":[],"source":["# Your code here: (￣▽￣)/♫•*¨*•.¸¸♪"]},{"cell_type":"markdown","metadata":{"id":"SEPN9wPqs91-"},"source":["__Задание 6. (Максимум 1 балл)__\n","\n","Реализуйте класс ядровой Ridge регрессии (Лекция 13, $\\S 1.2$), для оптимизации используте градиентный спуск, а не аналитическую формулу. Также подумайте о том, как в формулах правильно учесть свободный коэффициент. Затем адаптируйте вашу реализацию RFF под задачу регрессии. Сравните вашу ядровую регрессию и RFF на синтетических данных."]},{"cell_type":"markdown","metadata":{"id":"bjTwDlx6s91-"},"source":["Функция потерь:\n","$$\n","Q(w) = \\frac{1}{2} ||\\Phi \\Phi^T w - y||^2 + \\frac{\\lambda}{2} w^T \\Phi \\Phi^T w \\rightarrow \\min_w,\n","$$\n","где $\\Phi \\Phi^T = K$, $K = (k(x_i, x_j))_{i, j = 1}^{\\ell}$.\n","\n","Предсказание:\n","$\n","y(x) = k(x)^T w,\n","$\n","где $k(x)$ — вектор функций ядра от пар объектов $(x, x_i)_{i=1}^{\\ell}$.\n","\n","Вы можете изменять представленный ниже шаблон по своему усмотрению."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o7qmtwRZs91-"},"outputs":[],"source":["import numpy as np\n","from sklearn.base import RegressorMixin\n","from sklearn.gaussian_process.kernels import RBF\n","\n","class KernelRidgeRegression(RegressorMixin):\n","    \"\"\"\n","    Kernel Ridge regression class\n","    \"\"\"\n","\n","    def __init__(self,\n","        lr=0.01,\n","        regularization=1.,\n","        tolerance=1e-2,\n","        max_iter=1000,\n","        batch_size=64,\n","        kernel_scale=1.\n","    ):\n","        \"\"\"\n","        :param lr: learning rate\n","        :param regularization: regularization coefficient\n","        :param tolerance: stopping criterion for square of euclidean norm of weight difference\n","        :param max_iter: stopping criterion for iterations\n","        :param batch_size: size of the batches used in gradient descent steps\n","        :parame kernel_scale: length scale in RBF kernel formula\n","        \"\"\"\n","\n","        self.lr: float = lr\n","        self.regularization: float = regularization\n","        self.w: np.ndarray | None = None\n","\n","        self.tolerance: float = tolerance\n","        self.max_iter: int = max_iter\n","        self.batch_size: int = batch_size\n","        self.loss_history: list[float] = []\n","        self.kernel = RBF(kernel_scale)\n","\n","    def calc_loss(self, x: np.ndarray, y: np.ndarray) -> float:\n","        \"\"\"\n","        Calculating loss for x and y dataset\n","        :param x: features array\n","        :param y: targets array\n","        \"\"\"\n","        raise NotImplementedError\n","\n","    def calc_grad(self, x: np.ndarray, y: np.ndarray) -> float:\n","        \"\"\"\n","        Calculating gradient for x and y dataset\n","        :param x: features array\n","        :param y: targets array\n","        \"\"\"\n","        raise NotImplementedError\n","\n","    def fit(self, x: np.ndarray, y: np.ndarray) -> \"KernelRidgeRegression\":\n","        \"\"\"\n","        Fitting weights for x and y dataset\n","        :param x: features array\n","        :param y: targets array\n","        :return: self\n","        \"\"\"\n","        raise NotImplementedError\n","\n","    def predict(self, x: np.ndarray) -> np.ndarray:\n","        \"\"\"\n","        Predicting targets for x dataset\n","        :param x: features array\n","        :return: prediction: np.ndarray\n","        \"\"\"\n","        raise NotImplementedError"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}